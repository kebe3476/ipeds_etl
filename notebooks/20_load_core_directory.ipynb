{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ce9c0a-12b6-4567-b6b0-9c3617bd1487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ project root: C:\\Users\\keith\\Documents\\ipeds_etl\\ipeds_etl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/keith/Documents/ipeds_etl/ipeds_etl')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "def add_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Add the repo root (the folder that contains 'etl/' and 'sql/') to sys.path\n",
    "    so `from etl... import ...` works no matter where the notebook lives.\n",
    "    \"\"\"\n",
    "    cwd = Path.cwd()\n",
    "    for p in (cwd, *cwd.parents):\n",
    "        if (p / \"etl\").is_dir() and (p / \"sql\").exists():\n",
    "            if str(p) not in sys.path:\n",
    "                sys.path.insert(0, str(p))\n",
    "            print(\"✓ project root:\", p)\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find project root with 'etl' and 'sql'.\")\n",
    "\n",
    "add_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f82271-b66a-4b09-99b8-0625cd05c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB URL: postgresql+psycopg2://ipeds_loader:***@localhost:5432/ipeds_db\n",
      "Ping: ('ipeds_db', 'ipeds_loader')\n",
      "Schemas: ['ipeds_core', 'ipeds_dim', 'ipeds_raw', 'ipeds_vw', 'public']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.engine import make_url\n",
    "from etl.config import settings\n",
    "from etl.db import ping, list_ipeds_schemas\n",
    "\n",
    "print(\"DB URL:\", make_url(settings.DATABASE_URL).render_as_string(hide_password=True))\n",
    "print(\"Ping:\", ping())\n",
    "print(\"Schemas:\", list_ipeds_schemas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d087ca-22d0-454f-b664-e13f7860ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = \"directory\"\n",
    "YEARS = [2022]   # you can add more later, e.g. [2020, 2021, 2022, 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1251f2fd-d30e-4591-8771-d737c82c16d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ensured core table ipeds_core.directory\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Core table creation (DDL)\n",
    "# -----------------------------\n",
    "from sqlalchemy import text\n",
    "from etl.db import get_sqlalchemy_engine\n",
    "from etl.registry import get_endpoint_config\n",
    "\n",
    "def ensure_core_table(endpoint: str) -> None:\n",
    "    \"\"\"\n",
    "    Ensure a typed, de-duplicated *core* table exists for an endpoint.\n",
    "\n",
    "    What this creates:\n",
    "    - A table named: ipeds_core.{endpoint}\n",
    "    - Columns and SQL types come from etl/registry.py (single source of truth)\n",
    "    - PRIMARY KEY (unitid, year) to prevent duplicates for a given year\n",
    "    - A couple of helpful indexes used constantly by analysts/joins\n",
    "\n",
    "    Why this is in code (and not a .sql file):\n",
    "    - Keeps schema definition close to the ETL mapping logic\n",
    "    - Lets us evolve columns per endpoint in one place (the registry)\n",
    "    - Safe to run repeatedly: all DDL uses IF NOT EXISTS (idempotent)\n",
    "    \"\"\"\n",
    "    # Pull endpoint definition (columns + types, mapper, etc.)\n",
    "    cfg = get_endpoint_config(endpoint)\n",
    "    table = f\"ipeds_core.{endpoint}\"\n",
    "\n",
    "    # Turn {\"unitid\": \"INTEGER NOT NULL\", ...} into \"unitid INTEGER NOT NULL, ...\"\n",
    "    cols_sql = [f\"{col} {sqltype}\" for col, sqltype in cfg[\"schema\"].items()]\n",
    "    cols_block = \",\\n        \".join(cols_sql)\n",
    "\n",
    "    # DDL notes:\n",
    "    # - PRIMARY KEY matches our upsert conflict target later.\n",
    "    # - year index: common filter (dashboards by year).\n",
    "    # - state_abbr index: common filter/aggregation (slicing by state).\n",
    "    ddl = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table} (\n",
    "        {cols_block},\n",
    "        PRIMARY KEY (unitid, year)\n",
    "    );\n",
    "    CREATE INDEX IF NOT EXISTS {endpoint}_year_idx  ON {table} (year);\n",
    "    CREATE INDEX IF NOT EXISTS {endpoint}_state_idx ON {table} (state_abbr);\n",
    "    \"\"\"\n",
    "\n",
    "    eng = get_sqlalchemy_engine()\n",
    "\n",
    "    # Execute each DDL statement in a single transaction.\n",
    "    # We split on ';' because we're issuing multiple statements (DDL + indexes).\n",
    "    with eng.begin() as cx:\n",
    "        for stmt in ddl.strip().split(\";\"):\n",
    "            s = stmt.strip()\n",
    "            if s:\n",
    "                cx.execute(text(s))\n",
    "\n",
    "    print(f\"✓ ensured core table {table}\")\n",
    "\n",
    "\n",
    "# Example: make sure the table exists for the current endpoint constant\n",
    "ensure_core_table(ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a90be4-d0d8-49e8-9738-ce4dba145247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ directory: upserted 6256 rows for year 2022 into ipeds_core.directory\n",
      "TOTAL rows processed: 6256\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Raw → Core load (UPSERT)\n",
    "# -----------------------------\n",
    "from typing import List, Dict, Any\n",
    "from sqlalchemy import text\n",
    "from etl.db import get_sqlalchemy_engine\n",
    "from etl.mappers.directory import map_directory_row\n",
    "from etl.registry import get_endpoint_config\n",
    "\n",
    "def load_core_for_year(endpoint: str, year: int) -> int:\n",
    "    \"\"\"\n",
    "    Move one year's worth of data from ipeds_raw → ipeds_core for a given endpoint.\n",
    "\n",
    "    Pipeline steps (per year):\n",
    "    1) Read raw JSONB pages, flatten them to individual JSON objects (1 per institution)\n",
    "       using jsonb_array_elements + LATERAL.\n",
    "    2) Map/clean each raw JSON dict with the endpoint-specific mapper (handles types,\n",
    "       trims strings, converts magic -1/-2/-3 to NULL, etc.).\n",
    "    3) UPSERT into ipeds_core.{endpoint} on (unitid, year), updating non-PK columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int : number of rows written (length of the mapped list).\n",
    "    \"\"\"\n",
    "    cfg = get_endpoint_config(endpoint)\n",
    "    table = f\"ipeds_core.{endpoint}\"\n",
    "\n",
    "    # Preserve a deterministic column order for the INSERT.\n",
    "    # This must match what the mapper outputs.\n",
    "    schema_cols: List[str] = list(cfg[\"schema\"].keys())\n",
    "\n",
    "    # --- 1) Flatten raw JSON payloads into row-wise JSON (one dict per record) ---\n",
    "    # We read from ipeds_raw.{endpoint}_raw where each row is a *page* with a JSONB\n",
    "    # array called payload. jsonb_array_elements() expands that array into \"elem\".\n",
    "    # CROSS JOIN LATERAL is the idiomatic way to iterate JSON arrays in Postgres.\n",
    "    eng = get_sqlalchemy_engine()\n",
    "    flatten_sql = text(f\"\"\"\n",
    "        SELECT elem::jsonb AS row\n",
    "        FROM ipeds_raw.{endpoint}_raw r\n",
    "        CROSS JOIN LATERAL jsonb_array_elements(r.payload) AS elem\n",
    "        WHERE r.year = :y\n",
    "        ORDER BY r.page_number\n",
    "    \"\"\")\n",
    "    with eng.connect() as cx:\n",
    "        raw_rows: List[Dict[str, Any]] = [r[0] for r in cx.execute(flatten_sql, {\"y\": year}).fetchall()]\n",
    "\n",
    "    # --- 2) Map/clean raw dicts to our core schema ---\n",
    "    # The mapper returns a dict with exactly the keys in schema_cols (plus NULLs as needed).\n",
    "    # If you add fields to the registry, update the mapper to provide those keys.\n",
    "    mapped: List[Dict[str, Any]] = [map_directory_row(d) for d in raw_rows]\n",
    "\n",
    "    # --- 3) Build an UPSERT that updates all non-PK columns when the row already exists ---\n",
    "    # We exclude PK columns from the SET list (unitid, year) since those define the row.\n",
    "    set_cols = [c for c in schema_cols if c not in (\"unitid\", \"year\")]\n",
    "\n",
    "    # VALUES uses named parameters like :inst_name; we feed a list[dict] to cx.execute().\n",
    "    # ON CONFLICT target must match the table's PRIMARY KEY.\n",
    "    # We do a full-field overwrite on conflict — this is fine because the mapper\n",
    "    # always produces authoritative values for that (unitid, year).\n",
    "    insert_sql = text(f\"\"\"\n",
    "        INSERT INTO {table} ({\", \".join(schema_cols)})\n",
    "        VALUES ({\", \".join(\":\"+c for c in schema_cols)})\n",
    "        ON CONFLICT (unitid, year) DO UPDATE\n",
    "        SET {\", \".join(f\"{c}=EXCLUDED.{c}\" for c in set_cols)}\n",
    "    \"\"\")\n",
    "\n",
    "    # --- 4) Execute in a single transaction for atomicity & speed ---\n",
    "    with eng.begin() as cx:\n",
    "        # Passing the whole list allows SQLAlchemy to do an executemany() style insert.\n",
    "        cx.execute(insert_sql, mapped)\n",
    "\n",
    "    print(f\"✓ {endpoint}: upserted {len(mapped)} rows for year {year} into {table}\")\n",
    "    return len(mapped)\n",
    "\n",
    "\n",
    "# Example driver: load several years for the selected endpoint\n",
    "total = 0\n",
    "for y in YEARS:\n",
    "    total += load_core_for_year(ENDPOINT, y)\n",
    "print(\"TOTAL rows processed:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc3165-79f5-4bde-b79b-9c20d0b620f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
